TODO

- Model Flex-PRET pipeline

- Put back data dependencies rules

- Model branches

- Compute maximal delay an instruction can experience

NOTE on the design of the abstract models

- We assume branch are removed from ID (except Patmos) or EX (Patmos)

- We assume nop instruction except for Patmos

- We model no ST stage for Patmos (because it has no ST stage)

- We assume branch generating cache misses in Patmos as being part of
  load instructions

- We do not model the data dependencies as it has nothing to do with
  timing anomalies and would only artificially increase the number of
  states (besides Patmos assumes no data dependencies, it is the job
  of the compiler ...).

- Branch behavior? Currently, we assume that the branch is not taken (fall through)
  So pipeline flush is useless ...
  => If we want to explore both branches, we should ...

RESULTS

- Demonstrate no timing anomaly for SIC: done, BMC needed is 23
- Demonstrate no timing anomaly for Patmos: done, BMC needed is 14
- Demonstrate no timing anomaly for PRET: done, BMC needed is 10
- Demonstrate timing anomalies for inorder:
  stalling before and without stalling variants => done, BMC needed is 31
  stalling whole pipeline => done, BMC needed is 33 (I was expected an higher value)
- Demonstrate no timing anomay for SIC without no store pending when in EX stage
- Checked the behavior of previous and current for simultaneous willing of transition
  (cache miss, cache hit)

NOTES

- We were unable to model using an array of record using UCLID. Arrays
  of values works well, but we want to update a subset of an element
  of an array -> TLA+ seems better here. Similary we are unable to use
  prime value easily in a next block as we need to update a full
  variable. It is not allowed to update a field of a record for
  instance ==> one solution would be to model an instruction as a flat
  structure. This means that we have a lot of parameters to set when
  creating an instance of an instruction but that does not hurt ...


- Check the use of store_pending rule => For current to reach stage
  EX, this implies a cache hit in IF or no mempending. When current
  reach stage EX, it can only go in MEM. If it is a hit in MEM, it
  will go in MEM for 1 cycle and then directly in post if a store (as
  no WB stage), while in WB and then in post if a load.

  If no store_pending, ST is free and MEM potentially.  Then, if
  current is a store => no problem as ST is free: current can enter in
  the MEM stage it will not be blocked after. If current is a load,
  MEM may potentially be blocked: the willbefree rule will prevent any
  way the instruction to progress. Removing no storepending, ST may
  not be free as well. The current instruction cannot enter in the MEM
  if previous is a load (willbefree will prevent this in this case as
  well), so no problem. However, if previous is a store => when
  current is a load, current can enter in MEM and no problem. If
  current is a store, then current can enter in MEM and later being
  blocked in MEM while ST is occupied by previous.
  => So putting  no store pending is to allow only 1 single outstanding store
  
  => It has nothing to do with the timing anomaly if we assume that a
  store stays in the MEM stage for the time needed to pipeline stores. 

- a ST stage ensures that store accesses are performed in a asynchronous way.
  SIC paper models a write-allocate policy on write miss. No-write allocate policy
  would mean that MEM delays are ignored
  Note: If you think about the K1 processor => writes are never blocked ==>
  An infinite number of instructions can be in the ST stage => be careful for the sanity check
  For load, K1 as x loads in parallel possible, meaning that a timing anomay occurs
  if the number of instruction in MEM and IF stages is higher than x

- stores are asynchronous with the pipeline => be careful
  Write Allocate vs. No write allocate
  Write through vs. write back 
  CASE: Write back (=> in fact we cannot model this behavior by looking at the pipeline only)
  
   	and no write allocate => the write access will occur later on
   	the shared bus between instruction and data ...

	nothing
	
	and write allocate => This can potentially lead to evict data
	from the private cache (but this will occur later one) and
	load the data to be written in the cache, meaning that it is
	similar to a load operation and later on write ...

	write, load
	
  CASE: Write through => (usually assume no write allocate)

  	and no write allocate => the write access will access
  	immediately and thus request the shared bus (as the load op
  	but for a shorter amount of time as no need to wait for the
  	ack) => 

	and write allocate => a write for any potential evicted
	data, then a load, then a write

- While in WCET 2018 we were targeting scheduling timing anomalies: no
  other change than the length of a pivot instruction. This can be
  generalized to several instructions which are called speculation
  timing anomalies. Here we target speculation timing anomalies =>
  depending on the length of a pivot instruction, additional effects
  from the microarchitecture (access to the bus) impacts the other
  instructions.

The monotonicity assumption means that when uncertain information is
processed by a WCET analysis ap- proach, it is assumed that a longer
latency for an instruction necessarily imposes an at least equal or
longer (bounded by the amount of the latency change) execution time
for the overall instruction sequence under consideration.
